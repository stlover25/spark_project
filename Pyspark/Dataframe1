import collections

from pyspark import StorageLevel
from pyspark.sql import Row
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql import functions
from pyspark.sql.types import *
from pyspark.sql.window import Window
import time


#스파크 세션을 사용하면 DataFrame 생성, DataFrame을 테이블로 등록, SQL 실행, 테이블 캐시, 패켓 파일 읽기 등을 할 수 있다. 
#다음은 스파크 세션을 생성한 빌드 패턴 예시이다. 

spark = SparkSession \
    .builder \                                                            #sparksession의 빌더
    .appName("sample") \                                                  #app의 이름
    .master("local[*]") \                                                 #연결할 스파크 마스터 URL을 설정
    .config("spark.sql.warehouse.dir", "file:///Users/beginspark/Temp/") \ #(configuration property의 key값, configuration property의 value값)
    .config("spark.driver.host", "127.0.0.1") \
    .getOrCreate()                                    #기존 스파크 세션을 가져오거나 기존 스파크 세션이 없는 경우 이 빌더에 설정된 옵션을 기반으로 새 스파크 세션을 만듦


sample_dataframe = spark.range(1000).toDF("number") 

#sample_dataframe의 number가 짝수인 로우들을 찾는 트렌스포메이션
sample_tf = sample_dataframe.where("number%2 =0")

